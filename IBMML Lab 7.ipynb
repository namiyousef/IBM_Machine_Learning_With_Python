{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Machine Learning Course Lab 7 - Logistic Regression\n",
    "\n",
    "This is my own attempt at Lab 7 of 'Machine Learning with Python' by IBM on Coursera. It includes my own insight when solving problems. The method of analysis presented here is far more rigorous than that required by the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" importing necessary packages \"\"\"\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-12 16:06:29--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/ChurnData.csv\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 36144 (35K) [text/csv]\n",
      "Saving to: ‘ChurnData.csv’\n",
      "\n",
      "ChurnData.csv       100%[===================>]  35.30K   121KB/s    in 0.3s    \n",
      "\n",
      "2020-08-12 16:06:31 (121 KB/s) - ‘ChurnData.csv’ saved [36144/36144]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.030</td>\n",
       "      <td>3.240</td>\n",
       "      <td>4.787</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.110</td>\n",
       "      <td>3.157</td>\n",
       "      <td>3.611</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.065</td>\n",
       "      <td>3.240</td>\n",
       "      <td>2.833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.872</td>\n",
       "      <td>3.314</td>\n",
       "      <td>4.942</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.553</td>\n",
       "      <td>3.248</td>\n",
       "      <td>4.143</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "5    68.0  52.0     17.0   120.0  1.0    24.0    0.0       1.0       0.0   \n",
       "6    42.0  40.0      7.0    37.0  2.0     8.0    1.0       1.0       1.0   \n",
       "7     9.0  21.0      1.0    17.0  2.0     2.0    0.0       0.0       0.0   \n",
       "8    35.0  50.0     26.0   140.0  2.0    21.0    0.0       1.0       0.0   \n",
       "9    49.0  51.0     27.0    63.0  4.0    19.0    0.0       1.0       0.0   \n",
       "\n",
       "   longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0     4.40  ...    1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1     9.45  ...    0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2     6.30  ...    0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3     6.05  ...    1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4     7.10  ...    0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "5    20.70  ...    0.0       0.0       0.0     0.0    0.0    3.030    3.240   \n",
       "6     8.25  ...    0.0       1.0       1.0     1.0    1.0    2.110    3.157   \n",
       "7     2.90  ...    0.0       0.0       0.0     0.0    0.0    1.065    3.240   \n",
       "8     6.50  ...    0.0       0.0       1.0     1.0    0.0    1.872    3.314   \n",
       "9    12.85  ...    0.0       1.0       1.0     0.0    1.0    2.553    3.248   \n",
       "\n",
       "   lninc  custcat  churn  \n",
       "0  4.913      4.0    1.0  \n",
       "1  3.497      1.0    1.0  \n",
       "2  3.401      3.0    0.0  \n",
       "3  4.331      4.0    0.0  \n",
       "4  4.382      3.0    0.0  \n",
       "5  4.787      1.0    0.0  \n",
       "6  3.611      4.0    0.0  \n",
       "7  2.833      1.0    0.0  \n",
       "8  4.942      3.0    0.0  \n",
       "9  4.143      2.0    0.0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Importing the data set \"\"\"\n",
    "\n",
    "!wget -O ChurnData.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/ChurnData.csv\n",
    "df = pd.read_csv(\"ChurnData.csv\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 28 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   tenure    200 non-null    float64\n",
      " 1   age       200 non-null    float64\n",
      " 2   address   200 non-null    float64\n",
      " 3   income    200 non-null    float64\n",
      " 4   ed        200 non-null    float64\n",
      " 5   employ    200 non-null    float64\n",
      " 6   equip     200 non-null    float64\n",
      " 7   callcard  200 non-null    float64\n",
      " 8   wireless  200 non-null    float64\n",
      " 9   longmon   200 non-null    float64\n",
      " 10  tollmon   200 non-null    float64\n",
      " 11  equipmon  200 non-null    float64\n",
      " 12  cardmon   200 non-null    float64\n",
      " 13  wiremon   200 non-null    float64\n",
      " 14  longten   200 non-null    float64\n",
      " 15  tollten   200 non-null    float64\n",
      " 16  cardten   200 non-null    float64\n",
      " 17  voice     200 non-null    float64\n",
      " 18  pager     200 non-null    float64\n",
      " 19  internet  200 non-null    float64\n",
      " 20  callwait  200 non-null    float64\n",
      " 21  confer    200 non-null    float64\n",
      " 22  ebill     200 non-null    float64\n",
      " 23  loglong   200 non-null    float64\n",
      " 24  logtoll   200 non-null    float64\n",
      " 25  lninc     200 non-null    float64\n",
      " 26  custcat   200 non-null    float64\n",
      " 27  churn     200 non-null    int64  \n",
      "dtypes: float64(27), int64(1)\n",
      "memory usage: 43.9 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.854</td>\n",
       "      <td>3.199</td>\n",
       "      <td>4.419</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>34.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.792</td>\n",
       "      <td>3.332</td>\n",
       "      <td>3.178</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>6.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.348</td>\n",
       "      <td>3.168</td>\n",
       "      <td>3.850</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.70</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.163</td>\n",
       "      <td>3.866</td>\n",
       "      <td>3.219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>61.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.824</td>\n",
       "      <td>3.240</td>\n",
       "      <td>5.247</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0      11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1      33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2      23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3      38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4       7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "..      ...   ...      ...     ...  ...     ...    ...       ...       ...   \n",
       "195    55.0  44.0     24.0    83.0  1.0    23.0    0.0       1.0       0.0   \n",
       "196    34.0  23.0      3.0    24.0  1.0     7.0    0.0       1.0       0.0   \n",
       "197     6.0  32.0     10.0    47.0  1.0    10.0    0.0       1.0       0.0   \n",
       "198    24.0  30.0      0.0    25.0  4.0     5.0    0.0       1.0       1.0   \n",
       "199    61.0  50.0     16.0   190.0  2.0    22.0    1.0       1.0       1.0   \n",
       "\n",
       "     longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0       4.40  ...    1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1       9.45  ...    0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2       6.30  ...    0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3       6.05  ...    1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4       7.10  ...    0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "..       ...  ...    ...       ...       ...     ...    ...      ...      ...   \n",
       "195    17.35  ...    0.0       0.0       0.0     1.0    0.0    2.854    3.199   \n",
       "196     6.00  ...    0.0       0.0       1.0     1.0    0.0    1.792    3.332   \n",
       "197     3.85  ...    0.0       0.0       1.0     1.0    0.0    1.348    3.168   \n",
       "198     8.70  ...    1.0       1.0       1.0     1.0    1.0    2.163    3.866   \n",
       "199    16.85  ...    0.0       1.0       0.0     0.0    1.0    2.824    3.240   \n",
       "\n",
       "     lninc  custcat  churn  \n",
       "0    4.913      4.0      1  \n",
       "1    3.497      1.0      1  \n",
       "2    3.401      3.0      0  \n",
       "3    4.331      4.0      0  \n",
       "4    4.382      3.0      0  \n",
       "..     ...      ...    ...  \n",
       "195  4.419      3.0      0  \n",
       "196  3.178      3.0      0  \n",
       "197  3.850      3.0      0  \n",
       "198  3.219      4.0      1  \n",
       "199  5.247      2.0      0  \n",
       "\n",
       "[200 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.shape\n",
    "\n",
    "#the logistic regression functions require that the target variable is an integer, from df.info() we\n",
    "#see that is it a float, so we must change it to an integer\n",
    "\n",
    "df[['churn']] = df[['churn']].astype('int')\n",
    "\n",
    "#converting from df to array\n",
    "columns = df.columns.values\n",
    "columns = np.delete(columns,[len(columns)-1])\n",
    "\n",
    "X = np.asarray(df[columns]) #note, the IBM course only uses ['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']\n",
    "#in the X data, let's see what happens when we include all of them!\n",
    "\n",
    "y = np.asarray(df[['churn']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.08839657  0.14067695 -0.75495974 ...  0.03858151  0.88452453\n",
      "  -0.44114529]\n",
      " [ 0.85677355  0.06401376 -1.14970993 ...  2.80688971  0.07458016\n",
      "   1.41630855]\n",
      " [ 0.94942275  0.9073088   1.12010367 ...  0.03858151  0.87120307\n",
      "  -0.44114529]\n",
      " ...\n",
      " [ 0.71779974  0.60065606  1.71222895 ...  0.03858151 -0.19184891\n",
      "  -1.36987221]\n",
      " [-1.50578124 -0.7026181  -0.75495974 ... -1.6844763   0.14518386\n",
      "   1.41630855]\n",
      " [-0.53296457 -0.47262854 -0.16283445 ...  0.03858151 -0.31573843\n",
      "  -0.44114529]]\n"
     ]
    }
   ],
   "source": [
    "#normalize data set\n",
    "\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "X = pp.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts( X, y, test_size=0.2, random_state=4)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - Logistic Reg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 0 0 0 1\n",
      " 0 0 1]\n",
      "[[0], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0]]\n",
      "[[0.60722328 0.39277672]\n",
      " [0.61809654 0.38190346]\n",
      " [0.58411229 0.41588771]\n",
      " [0.65417657 0.34582343]\n",
      " [0.57846128 0.42153872]\n",
      " [0.60571723 0.39428277]\n",
      " [0.49465243 0.50534757]\n",
      " [0.63096405 0.36903595]\n",
      " [0.37261192 0.62738808]\n",
      " [0.57501555 0.42498445]\n",
      " [0.43796261 0.56203739]\n",
      " [0.56949003 0.43050997]\n",
      " [0.52659009 0.47340991]\n",
      " [0.38212909 0.61787091]\n",
      " [0.68571532 0.31428468]\n",
      " [0.52974013 0.47025987]\n",
      " [0.49534501 0.50465499]\n",
      " [0.54486783 0.45513217]\n",
      " [0.42671406 0.57328594]\n",
      " [0.58188784 0.41811216]\n",
      " [0.50068924 0.49931076]\n",
      " [0.41069809 0.58930191]\n",
      " [0.80418638 0.19581362]\n",
      " [0.34302289 0.65697711]\n",
      " [0.43713534 0.56286466]\n",
      " [0.75147663 0.24852337]\n",
      " [0.39496994 0.60503006]\n",
      " [0.42173992 0.57826008]\n",
      " [0.53615371 0.46384629]\n",
      " [0.82329995 0.17670005]\n",
      " [0.74876986 0.25123014]\n",
      " [0.60601763 0.39398237]\n",
      " [0.31608844 0.68391156]\n",
      " [0.72967456 0.27032544]\n",
      " [0.70767479 0.29232521]\n",
      " [0.60199123 0.39800877]\n",
      " [0.36515497 0.63484503]\n",
      " [0.60399177 0.39600823]\n",
      " [0.84359251 0.15640749]\n",
      " [0.37455692 0.62544308]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousefnami/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n",
    "LR\n",
    "\n",
    "#regularization is a technique used to prevent overfitting. C is the inverse of it's strength.\n",
    "\n",
    "#higher C == weaker regulairzation so more overfit\n",
    "#solver is the type of algorithm that you want the model to use\n",
    "\n",
    "yhat = LR.predict(X_test)\n",
    "print(yhat) #the output is essentially the designated 'classes that it predicted for the X_test'\n",
    "\n",
    "#compare them with y_test\n",
    "print(y_test.tolist())\n",
    "#there are many mispredicted values!!!!\n",
    "\n",
    "yhat_prob = LR.predict_proba(X_test)\n",
    "print(yhat_prob) # the outcome is very odd... so the first one is the prob. of class 0, and the second of class 1?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yousefnami/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:664: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "print(jaccard_similarity_score(y_test, yhat))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, yhat))\n",
    "\n",
    "#why are they both the same lmao...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  8]\n",
      " [ 7 18]]\n",
      "Confusion matrix, without normalization\n",
      "[[ 7  8]\n",
      " [ 7 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.71        25\n",
      "           1       0.50      0.47      0.48        15\n",
      "\n",
      "    accuracy                           0.62        40\n",
      "   macro avg       0.60      0.59      0.59        40\n",
      "weighted avg       0.62      0.62      0.62        40\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6328239710889156"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" This portion has been copied from the IBM course without any of my own input\"\"\"\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "print(confusion_matrix(y_test, yhat, labels=[1,0]))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')\n",
    "\n",
    "\n",
    "\n",
    "print (classification_report(y_test, yhat))\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, yhat_prob)\n",
    "\n",
    "\n",
    "#NOTE: my method of using ALL the data yields a lower accuracy score than what the IBM lab gets.\n",
    "#I wonder, is there any sort of data exploration that you can do for classification problems?\n",
    "\n",
    "#I presume one thing you can try is, if you plot the distributions for churn = 1 and 0, and then the dist\n",
    "#for all the other ones, I presume those with similar distributions will be related? Not sure...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This is a difficult, yet very interesting topic. Need to start playing around with it myself :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
